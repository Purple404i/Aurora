# Advanced Training Guide: Coding, Science & PicoGK

This guide explains how to train Aurora on specialized technical domains using automated data acquisition from Hugging Face and LEAP71 repositories.

## 1. Automated Data Fetching

Aurora now supports automatic acquisition of high-quality technical datasets. When you prepare the data, the system can fetch:

- **Hugging Face Datasets**: Specialized science and coding datasets.
- **LEAP71 Repositories**: Full documentation and C# code examples for PicoGK, ShapeKernel, and LatticeLibrary.

All acquired data is stored in the `books/` folder as `.txt` files, allowing you to inspect the training material before starting the fine-tuning.

### Configuration
You can customize the sources in `config.py`:

```python
# Advanced Training Datasets (Hugging Face Hub)
HF_DATASETS = [
    {"repo": "sciq", "name": "science_qa"},
    {"repo": "camel-ai/physics", "name": "physics"},
    {"repo": "camel-ai/biology", "name": "biology"},
    {"repo": "camel-ai/chemistry", "name": "chemistry"},
    {"repo": "theblackcat102/evol-codealpaca-v1", "name": "coding"},
]

# LEAP71 Repositories for Documentation and Code Examples
LEAP71_REPOS = [
    "https://github.com/leap71/PicoGK",
    "https://github.com/leap71/LEAP71_ShapeKernel",
    "https://github.com/leap71/LEAP71_LatticeLibrary",
]
```

## 2. High-Quality Dataset Resources

For optimal performance in Aurora's core domains, we recommend the following datasets:

### Science & Engineering
- **SciQ**: 13,679 crowdsourced science exam questions. [huggingface.co/datasets/sciq](https://huggingface.co/datasets/sciq)
- **Camel-AI Technical**: Multi-turn technical conversations.
  - [Physics](https://huggingface.co/datasets/camel-ai/physics)
  - [Biology](https://huggingface.co/datasets/camel-ai/biology)
  - [Chemistry](https://huggingface.co/datasets/camel-ai/chemistry)

### Coding & Robotics
- **Evol-CodeAlpaca**: Complex coding instructions generated by LLMs. [huggingface.co/datasets/theblackcat102/evol-codealpaca-v1](https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1)
- **LEAP71 Repos**: The primary source for learning PicoGK C# syntax and computational engineering logic.

## 3. Running the Training

To start an advanced training run:

1.  **Install Dependencies**:
    ```bash
    pip install datasets
    ```
2.  **Start Training**:
    ```bash
    python train.py
    ```
    The script will automatically detect if remote fetching is needed and populate the `books/` folder.

3.  **Manual Data Prep** (Optional):
    If you want to fetch the data without starting the full training:
    ```bash
    python data_preparation.py
    ```

## 4. Optimizing for RobotCEM

Aurora is designed to be the "brain" of the [RobotCEM](https://github.com/Purple404i/RobotCEM) framework. To ensure it can accurately generate PicoGK geometry and Blender simulations:

- **Context Window**: Use `MAX_SEQ_LENGTH = 8192` if your hardware allows. Long code blocks require large context windows.
- **System Prompt**: Ensure the detailed `SYSTEM_PROMPT` in `config.py` is preserved, as it contains the reasoning protocols needed for RobotCEM's Design-Simulate-Fix loop.
- **Dataset Mix**: Balance your `books/` folder with roughly 40% science, 40% coding (PicoGK/C#), and 20% general robotics texts.

## 5. Troubleshooting

- **Download Timed Out**: Large datasets may take time. Ensure a stable internet connection.
- **Git Clone Errors**: Ensure `git` is installed and you have access to GitHub.
- **Hugging Face Errors**: Some datasets may require you to accept terms on their HF page or set an `HF_TOKEN`.

---

**Note**: After the first successful fetch, you can set `fetch_remote=False` in the `prepare_dataset` call within `train.py` to skip the download phase in future runs.
